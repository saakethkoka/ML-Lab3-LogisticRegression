{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('star_classification.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get rid of all rows which have class as GALAXY\n",
    "df = df[df['class'] != 'GALAXY']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('class', axis=1), df['class'], test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# encode the labels:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegressionBase:\n",
    "    def __init__(self, eta=0.1, iterations=20, C1=0.0001, C2 = 0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "    def __str__(self):\n",
    "        if(hasattr(self, 'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    @staticmethod\n",
    "    def _add_bias(X):   \n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return expit(theta)\n",
    "    def _get_gradient(self, X, y):\n",
    "        ydiff = y - self.predict_proba(X, add_bias = False).ravel()\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return gradient\n",
    "    def _get_gradient_L2(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return gradient\n",
    "    def _get_gradient_L1(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * self.C1\n",
    "        return gradient\n",
    "    def _get_gradient_elastic(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * self.C1\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return gradient\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.random.uniform(-1, 1, (num_features, 1))\n",
    "        for i in range(self.iters):\n",
    "            if(regularization == 'L1'):\n",
    "                grad = self._get_gradient_L1(Xb, y)\n",
    "            elif(regularization == 'L2'):\n",
    "                grad = self._get_gradient_L2(Xb, y)\n",
    "            elif(regularization == 'elastic'):\n",
    "                grad = self._get_gradient_elastic(Xb, y)\n",
    "            else:\n",
    "                grad = self._get_gradient(Xb, y)\n",
    "            self.w_ += grad*self.eta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegressionBase(eta=0.1, iterations=1000)\n",
    "model.fit(X_train, y_train, regularization='elastic')\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegressionSGD(LogisticRegressionBase):\n",
    "    def _get_gradient(self, X, y):\n",
    "        sample = int(np.random.rand()  * len(y))\n",
    "        ydiff = y[sample] - self.predict_proba(X[sample],add_bias=False)\n",
    "        gradient = X[sample] * ydiff[:, np.newaxis]\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "\n",
    "        return gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegressionNewtons(LogisticRegressionBase):\n",
    "    def _get_gradient(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_L1(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X # the second derivative of abs(x) evaluates to 0 so our hessian will simply be the one for the ordinary log likelihood\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X @ ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] += -1 * l1_der[1:] * self.C1\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_L2(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X - 2 * self.C2\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_elastic(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X - 2 * self.C2\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X @ ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_)\n",
    "        l1_der = self.w_ / np.abs(self.w_)\n",
    "        gradient[1:] += -1 * l1_der[1:] * self.C1\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return np.linalg.pinv(hessian) @ gradient\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegressionNewtons(eta=0.1, iterations=10)\n",
    "model.fit(X_train, y_train, regularization='L2')\n",
    "y_pred = model.predict(X_test)\n",
    "print(model)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import ma\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSLogisticRegression(LogisticRegressionBase):\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C1,C2):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C2*sum(w**2) + C1*sum(np.abs(w))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C1, C2):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C2\n",
    "        l1_der = w[1:] / np.abs(w[1:])\n",
    "        l1_der[w[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * C1\n",
    "\n",
    "        return -gradient\n",
    "\n",
    "\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        #modifying regularization params so we can use one objective function and gradient in all cases\n",
    "        if(regularization == 'L1'):\n",
    "            self.C2 = 0\n",
    "        elif(regularization == 'L2'):\n",
    "            self.C1 = 0\n",
    "        elif(regularization == 'elastic'):\n",
    "            pass\n",
    "        else:\n",
    "            self.C1 = 0\n",
    "            self.C2 = 0\n",
    "\n",
    "        self.w_ = fmin_bfgs(self.objective_function,\n",
    "                        np.zeros((num_features,1)),\n",
    "                        fprime=self.objective_gradient,\n",
    "                        args=(Xb,y, self.C1, self.C2),\n",
    "                        gtol=1e-03,\n",
    "                        maxiter=self.iters,\n",
    "                        disp=False)\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bfgslr = BFGSLogisticRegression(iterations=200, C2=0.001, C1=0.001)\n",
    "bfgslr.fit(X_train, y_train, regularization='elastic')\n",
    "print(bfgslr)\n",
    "print(accuracy_score(y_test, bfgslr.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class BFGSFromScratchLogisticRegression(LogisticRegressionBase):\n",
    "\n",
    "\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        if(regularization == 'L1'):\n",
    "            self.C2 = 0\n",
    "        elif(regularization == 'L2'):\n",
    "            self.C1 = 0\n",
    "        elif(regularization == 'elastic'):\n",
    "            pass\n",
    "        else:\n",
    "            self.C1 = 0\n",
    "            self.C2 = 0\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        self.inv_hessian = np.identity(num_features)\n",
    "        self.last_grad = np.zeros((num_features,1))\n",
    "        g = self.predict_proba(Xb, add_bias=False).ravel()\n",
    "        ydiff = y-g\n",
    "        self.last_grad = np.sum(Xb * ydiff[:, np.newaxis], axis=0)\n",
    "        self.last_grad = self.last_grad.reshape((num_features, 1))\n",
    "        self.last_grad[1:] += -2 * self.w_[1:] * self.C2\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        self.last_grad[1:] +=  -1 * l1_der * self.C1\n",
    "        self.last_grad = -self.last_grad\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            pk = -np.dot(self.inv_hessian, self.last_grad)\n",
    "            pk = pk.reshape((num_features, 1))\n",
    "            sk = self.eta * pk\n",
    "            self.w_ += sk\n",
    "            g = self.predict_proba(Xb, add_bias=False).ravel()\n",
    "            ydiff = y-g\n",
    "            curr_grad = np.sum(Xb * ydiff[:, np.newaxis], axis=0)\n",
    "            curr_grad = curr_grad.reshape((num_features, 1))\n",
    "            curr_grad[1:] += -2 * self.w_[1:] * self.C2\n",
    "            l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "            l1_der[self.w_[1:] == 0] = 0\n",
    "            curr_grad[1:] +=  -1 * l1_der * self.C1\n",
    "            curr_grad = -curr_grad\n",
    "            vk = curr_grad - self.last_grad\n",
    "            inv_hessian_num_1_1 = (sk.T @ vk) + self.inv_hessian\n",
    "            inv_hessian_num_1_2 = sk @ sk.T\n",
    "            inv_hessian_num_1 = inv_hessian_num_1_1 @ inv_hessian_num_1_2\n",
    "            inv_hessian_dom_1 = (sk.T @ vk) ** 2\n",
    "            inv_hessian_1 = inv_hessian_num_1 / inv_hessian_dom_1\n",
    "            inv_hessian_num_2 = (self.inv_hessian @ vk @ sk.T) + (sk @ vk.T @ self.inv_hessian)\n",
    "            inv_hessian_dom_2 =  sk.T @ vk\n",
    "            inv_hessian_2 = inv_hessian_num_2 / inv_hessian_dom_2\n",
    "            self.inv_hessian += inv_hessian_1 - inv_hessian_2\n",
    "            self.last_grad = curr_grad\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ -8.88627572]\n",
      " [  5.27509514]\n",
      " [  0.90772017]\n",
      " [ -0.53636115]\n",
      " [  4.44592337]\n",
      " [  2.95011535]\n",
      " [ -9.16086727]\n",
      " [-17.01320358]\n",
      " [  2.19833927]\n",
      " [  5.27584254]\n",
      " [  9.46448486]\n",
      " [  2.3352807 ]\n",
      " [  1.83822263]\n",
      " [ -1.66595187]\n",
      " [-89.71738737]\n",
      " [ -1.66584169]\n",
      " [  3.93691556]\n",
      " [  1.63839714]]\n",
      "0.9562322771544816\n"
     ]
    }
   ],
   "source": [
    "test_model = BFGSFromScratchLogisticRegression(iterations=100, eta=0.001)\n",
    "test_model.fit(X_train, y_train, regularization='elastic')\n",
    "print(test_model)\n",
    "print(accuracy_score(y_test, test_model.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta=.1, iters=10, C1=.001, C2=.0001, solver=\"default\", regularization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.solver = solver\n",
    "        self.classifiers = []\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get number of unique values of y\n",
    "        unique_classes = np.unique(y)\n",
    "        unique_classes.sort()\n",
    "        for target in unique_classes:\n",
    "            # Transform the data into binary classification, the taget class vs the rest\n",
    "            y_binary = np.where(y == target, 1, 0)\n",
    "            if self.solver == \"default\":\n",
    "                model = LogisticRegressionBase(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"sgd\":\n",
    "                model = LogisticRegressionSGD(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"newton\":\n",
    "                model = LogisticRegressionNewtons(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs':\n",
    "                model = BFGSLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs_scratch':\n",
    "                model = BFGSFromScratchLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            model.fit(X, y_binary, regularization=self.regularization)\n",
    "            self.classifiers.append(model)\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers]).T\n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for model in self.classifiers:\n",
    "            probs.append(model.predict_proba(X).reshape(len(X), 1))\n",
    "        return np.hstack(probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegression(iters=150000, eta=1, solver=\"sgd\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy of y_hat vs y_test with sklearn\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 5 µs, total: 8 µs\n",
      "Wall time: 4.77 µs\n",
      "Accuracy:  0.814\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(iters=10, eta=.1, solver=\"newton\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat_new = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat_new))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 4 µs, total: 7 µs\n",
      "Wall time: 3.1 µs\n",
      "0.95485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "SKmodel = SKLogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
    "SKmodel.fit(X_train, y_train)\n",
    "%time\n",
    "print(accuracy_score(y_true=y_test, y_pred=SKmodel.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}