{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('star_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             obj_ID       alpha      delta         u         g         r  \\\n0      1.237661e+18  135.689107  32.494632  23.87882  22.27530  20.39501   \n1      1.237665e+18  144.826101  31.274185  24.77759  22.83188  22.58444   \n2      1.237661e+18  142.188790  35.582444  25.26307  22.66389  20.60976   \n3      1.237663e+18  338.741038  -0.402828  22.13682  23.77656  21.61162   \n4      1.237680e+18  345.282593  21.183866  19.43718  17.58028  16.49747   \n...             ...         ...        ...       ...       ...       ...   \n99995  1.237679e+18   39.620709  -2.594074  22.16759  22.97586  21.90404   \n99996  1.237679e+18   29.493819  19.798874  22.69118  22.38628  20.45003   \n99997  1.237668e+18  224.587407  15.700707  21.16916  19.26997  18.20428   \n99998  1.237661e+18  212.268621  46.660365  25.35039  21.63757  19.91386   \n99999  1.237661e+18  196.896053  49.464643  22.62171  21.79745  20.60115   \n\n              i         z  run_ID  rerun_ID  cam_col  field_ID   spec_obj_ID  \\\n0      19.16573  18.79371    3606       301        2        79  6.543777e+18   \n1      21.16812  21.61427    4518       301        5       119  1.176014e+19   \n2      19.34857  18.94827    3606       301        2       120  5.152200e+18   \n3      20.50454  19.25010    4192       301        3       214  1.030107e+19   \n4      15.97711  15.54461    8102       301        3       137  6.891865e+18   \n...         ...       ...     ...       ...      ...       ...           ...   \n99995  21.30548  20.73569    7778       301        2       581  1.055431e+19   \n99996  19.75759  19.41526    7917       301        1       289  8.586351e+18   \n99997  17.69034  17.35221    5314       301        4       308  3.112008e+18   \n99998  19.07254  18.62482    3650       301        4       131  7.601080e+18   \n99999  20.00959  19.28075    3650       301        4        60  8.343152e+18   \n\n        class  redshift  plate    MJD  fiber_ID  \n0      GALAXY  0.634794   5812  56354       171  \n1      GALAXY  0.779136  10445  58158       427  \n2      GALAXY  0.644195   4576  55592       299  \n3      GALAXY  0.932346   9149  58039       775  \n4      GALAXY  0.116123   6121  56187       842  \n...       ...       ...    ...    ...       ...  \n99995  GALAXY  0.000000   9374  57749       438  \n99996  GALAXY  0.404895   7626  56934       866  \n99997  GALAXY  0.143366   2764  54535        74  \n99998  GALAXY  0.455040   6751  56368       470  \n99999  GALAXY  0.542944   7410  57104       851  \n\n[100000 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obj_ID</th>\n      <th>alpha</th>\n      <th>delta</th>\n      <th>u</th>\n      <th>g</th>\n      <th>r</th>\n      <th>i</th>\n      <th>z</th>\n      <th>run_ID</th>\n      <th>rerun_ID</th>\n      <th>cam_col</th>\n      <th>field_ID</th>\n      <th>spec_obj_ID</th>\n      <th>class</th>\n      <th>redshift</th>\n      <th>plate</th>\n      <th>MJD</th>\n      <th>fiber_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.237661e+18</td>\n      <td>135.689107</td>\n      <td>32.494632</td>\n      <td>23.87882</td>\n      <td>22.27530</td>\n      <td>20.39501</td>\n      <td>19.16573</td>\n      <td>18.79371</td>\n      <td>3606</td>\n      <td>301</td>\n      <td>2</td>\n      <td>79</td>\n      <td>6.543777e+18</td>\n      <td>GALAXY</td>\n      <td>0.634794</td>\n      <td>5812</td>\n      <td>56354</td>\n      <td>171</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.237665e+18</td>\n      <td>144.826101</td>\n      <td>31.274185</td>\n      <td>24.77759</td>\n      <td>22.83188</td>\n      <td>22.58444</td>\n      <td>21.16812</td>\n      <td>21.61427</td>\n      <td>4518</td>\n      <td>301</td>\n      <td>5</td>\n      <td>119</td>\n      <td>1.176014e+19</td>\n      <td>GALAXY</td>\n      <td>0.779136</td>\n      <td>10445</td>\n      <td>58158</td>\n      <td>427</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.237661e+18</td>\n      <td>142.188790</td>\n      <td>35.582444</td>\n      <td>25.26307</td>\n      <td>22.66389</td>\n      <td>20.60976</td>\n      <td>19.34857</td>\n      <td>18.94827</td>\n      <td>3606</td>\n      <td>301</td>\n      <td>2</td>\n      <td>120</td>\n      <td>5.152200e+18</td>\n      <td>GALAXY</td>\n      <td>0.644195</td>\n      <td>4576</td>\n      <td>55592</td>\n      <td>299</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.237663e+18</td>\n      <td>338.741038</td>\n      <td>-0.402828</td>\n      <td>22.13682</td>\n      <td>23.77656</td>\n      <td>21.61162</td>\n      <td>20.50454</td>\n      <td>19.25010</td>\n      <td>4192</td>\n      <td>301</td>\n      <td>3</td>\n      <td>214</td>\n      <td>1.030107e+19</td>\n      <td>GALAXY</td>\n      <td>0.932346</td>\n      <td>9149</td>\n      <td>58039</td>\n      <td>775</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.237680e+18</td>\n      <td>345.282593</td>\n      <td>21.183866</td>\n      <td>19.43718</td>\n      <td>17.58028</td>\n      <td>16.49747</td>\n      <td>15.97711</td>\n      <td>15.54461</td>\n      <td>8102</td>\n      <td>301</td>\n      <td>3</td>\n      <td>137</td>\n      <td>6.891865e+18</td>\n      <td>GALAXY</td>\n      <td>0.116123</td>\n      <td>6121</td>\n      <td>56187</td>\n      <td>842</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>1.237679e+18</td>\n      <td>39.620709</td>\n      <td>-2.594074</td>\n      <td>22.16759</td>\n      <td>22.97586</td>\n      <td>21.90404</td>\n      <td>21.30548</td>\n      <td>20.73569</td>\n      <td>7778</td>\n      <td>301</td>\n      <td>2</td>\n      <td>581</td>\n      <td>1.055431e+19</td>\n      <td>GALAXY</td>\n      <td>0.000000</td>\n      <td>9374</td>\n      <td>57749</td>\n      <td>438</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>1.237679e+18</td>\n      <td>29.493819</td>\n      <td>19.798874</td>\n      <td>22.69118</td>\n      <td>22.38628</td>\n      <td>20.45003</td>\n      <td>19.75759</td>\n      <td>19.41526</td>\n      <td>7917</td>\n      <td>301</td>\n      <td>1</td>\n      <td>289</td>\n      <td>8.586351e+18</td>\n      <td>GALAXY</td>\n      <td>0.404895</td>\n      <td>7626</td>\n      <td>56934</td>\n      <td>866</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>1.237668e+18</td>\n      <td>224.587407</td>\n      <td>15.700707</td>\n      <td>21.16916</td>\n      <td>19.26997</td>\n      <td>18.20428</td>\n      <td>17.69034</td>\n      <td>17.35221</td>\n      <td>5314</td>\n      <td>301</td>\n      <td>4</td>\n      <td>308</td>\n      <td>3.112008e+18</td>\n      <td>GALAXY</td>\n      <td>0.143366</td>\n      <td>2764</td>\n      <td>54535</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>1.237661e+18</td>\n      <td>212.268621</td>\n      <td>46.660365</td>\n      <td>25.35039</td>\n      <td>21.63757</td>\n      <td>19.91386</td>\n      <td>19.07254</td>\n      <td>18.62482</td>\n      <td>3650</td>\n      <td>301</td>\n      <td>4</td>\n      <td>131</td>\n      <td>7.601080e+18</td>\n      <td>GALAXY</td>\n      <td>0.455040</td>\n      <td>6751</td>\n      <td>56368</td>\n      <td>470</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>1.237661e+18</td>\n      <td>196.896053</td>\n      <td>49.464643</td>\n      <td>22.62171</td>\n      <td>21.79745</td>\n      <td>20.60115</td>\n      <td>20.00959</td>\n      <td>19.28075</td>\n      <td>3650</td>\n      <td>301</td>\n      <td>4</td>\n      <td>60</td>\n      <td>8.343152e+18</td>\n      <td>GALAXY</td>\n      <td>0.542944</td>\n      <td>7410</td>\n      <td>57104</td>\n      <td>851</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get rid of all rows which have class as GALAXY\n",
    "df = df[df['class'] != 'GALAXY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('class', axis=1), df['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# encode the labels:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionBase:\n",
    "    def __init__(self, eta=0.1, iterations=20, C1=0.0001, C2 = 0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "    def __str__(self):\n",
    "        if(hasattr(self, 'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    @staticmethod\n",
    "    def _add_bias(X):   \n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return expit(theta)\n",
    "    def _get_gradient(self, X, y):\n",
    "        ydiff = y - self.predict_proba(X, add_bias = False).ravel()\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return gradient\n",
    "    def _get_gradient_L2(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return gradient\n",
    "    def _get_gradient_L1(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * self.C1\n",
    "        return gradient\n",
    "    def _get_gradient_elastic(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * self.C1\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return gradient\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.random.uniform(-1, 1, (num_features, 1))\n",
    "        for i in range(self.iters):\n",
    "            if(regularization == 'L1'):\n",
    "                grad = self._get_gradient_L1(Xb, y)\n",
    "            elif(regularization == 'L2'):\n",
    "                grad = self._get_gradient_L2(Xb, y)\n",
    "            elif(regularization == 'elastic'):\n",
    "                grad = self._get_gradient_elastic(Xb, y)\n",
    "            else:\n",
    "                grad = self._get_gradient(Xb, y)\n",
    "            self.w_ += grad*self.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.33585\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionBase(eta=0.1, iterations=1000)\n",
    "model.fit(X_train, y_train, regularization='elastic')\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionSGD(LogisticRegressionBase):\n",
    "    def _get_gradient(self, X, y):\n",
    "        sample = int(np.random.rand()  * len(y))\n",
    "        ydiff = y[sample] - self.predict_proba(X[sample],add_bias=False)\n",
    "        gradient = X[sample] * ydiff[:, np.newaxis]\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionNewtons(LogisticRegressionBase):\n",
    "    def _get_gradient(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_L1(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X # the second derivative of abs(x) evaluates to 0 so our hessian will simply be the one for the ordinary log likelihood\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X @ ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] += -1 * l1_der[1:] * self.C1\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_L2(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X - 2 * self.C2\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_elastic(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X - 2 * self.C2\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X @ ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_)\n",
    "        l1_der = self.w_ / np.abs(self.w_)\n",
    "        gradient[1:] += -1 * l1_der[1:] * self.C1\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return np.linalg.pinv(hessian) @ gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-9.40343349e-02]\n",
      " [ 4.98958754e-01]\n",
      " [ 1.52642719e-01]\n",
      " [-2.54707938e-01]\n",
      " [ 1.89886327e+01]\n",
      " [-3.48716796e+01]\n",
      " [ 5.84916770e-01]\n",
      " [ 5.30809029e-01]\n",
      " [-1.79918575e+01]\n",
      " [-1.29219938e-01]\n",
      " [ 1.36337110e+02]\n",
      " [-3.15769960e-01]\n",
      " [-1.10324157e-01]\n",
      " [-3.93870990e-01]\n",
      " [-1.80952813e+00]\n",
      " [-1.58179032e-02]\n",
      " [ 2.75764412e-01]\n",
      " [-1.42204671e-01]]\n",
      "Accuracy:  0.9351497965725558\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionNewtons(eta=0.1, iterations=10)\n",
    "model.fit(X_train, y_train, regularization='L2')\n",
    "y_pred = model.predict(X_test)\n",
    "print(model)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "outputs": [],
   "source": [
    "from numpy import ma\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSLogisticRegression(LogisticRegressionBase):\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C1,C2):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C2*sum(w**2) + C1*sum(np.abs(w))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C1, C2):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C2\n",
    "        l1_der = w[1:] / np.abs(w[1:])\n",
    "        l1_der[w[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * C1\n",
    "\n",
    "        return -gradient\n",
    "\n",
    "\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        #modifying regularization params so we can use one objective function and gradient in all cases\n",
    "        if(regularization == 'L1'):\n",
    "            self.C2 = 0\n",
    "        elif(regularization == 'L2'):\n",
    "            self.C1 = 0\n",
    "        elif(regularization == 'elastic'):\n",
    "            pass\n",
    "        else:\n",
    "            self.C1 = 0\n",
    "            self.C2 = 0\n",
    "\n",
    "        self.w_ = fmin_bfgs(self.objective_function,\n",
    "                        np.zeros((num_features,1)),\n",
    "                        fprime=self.objective_gradient,\n",
    "                        args=(Xb,y, self.C1, self.C2),\n",
    "                        gtol=1e-03,\n",
    "                        maxiter=self.iters,\n",
    "                        disp=False)\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-1.63015499e+00]\n",
      " [-1.83140808e-01]\n",
      " [-1.00817179e-01]\n",
      " [-3.92314450e-02]\n",
      " [ 1.57771136e-02]\n",
      " [-3.85320072e-03]\n",
      " [ 1.19140957e-01]\n",
      " [-7.09587864e-01]\n",
      " [-3.65967384e-02]\n",
      " [-1.83144369e-01]\n",
      " [ 0.00000000e+00]\n",
      " [ 4.08749496e-03]\n",
      " [-1.47740638e-02]\n",
      " [-9.99228090e-03]\n",
      " [-6.03358052e+00]\n",
      " [-9.99305125e-03]\n",
      " [ 3.47034543e-01]\n",
      " [-1.52369997e-02]]\n",
      "0.9749722598939712\n"
     ]
    }
   ],
   "source": [
    "bfgslr = BFGSLogisticRegression(iterations=200, C2=0.001, C1=0.001)\n",
    "bfgslr.fit(X_train, y_train, regularization='elastic')\n",
    "print(bfgslr)\n",
    "print(accuracy_score(y_test, bfgslr.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "outputs": [],
   "source": [
    "class BFGSFromScratchLogisticRegression(LogisticRegressionBase):\n",
    "\n",
    "\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        if(regularization == 'L1'):\n",
    "            self.C2 = 0\n",
    "        elif(regularization == 'L2'):\n",
    "            self.C1 = 0\n",
    "        elif(regularization == 'elastic'):\n",
    "            pass\n",
    "        else:\n",
    "            self.C1 = 0\n",
    "            self.C2 = 0\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        self.inv_hessian = np.identity(num_features)\n",
    "        self.last_grad = np.zeros((num_features,1))\n",
    "        g = self.predict_proba(Xb, add_bias=False).ravel()\n",
    "        ydiff = y-g\n",
    "        self.last_grad = np.sum(Xb * ydiff[:, np.newaxis], axis=0)\n",
    "        self.last_grad = self.last_grad.reshape((num_features, 1))\n",
    "        self.last_grad[1:] += -2 * self.w_[1:] * self.C2\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        self.last_grad[1:] +=  -1 * l1_der * self.C1\n",
    "        for i in range(self.iters):\n",
    "            pk = self.inv_hessian @ self.last_grad\n",
    "            pk = pk.reshape((num_features, 1))\n",
    "            sk = self.eta * pk\n",
    "            self.w_ += sk\n",
    "            g = self.predict_proba(Xb, add_bias=False).ravel()\n",
    "            ydiff = y-g\n",
    "            curr_grad = np.sum(Xb * ydiff[:, np.newaxis], axis=0)\n",
    "            curr_grad = curr_grad.reshape((num_features, 1))\n",
    "            curr_grad[1:] += -2 * self.w_[1:] * self.C2\n",
    "            l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "            l1_der[self.w_[1:] == 0] = 0\n",
    "            curr_grad[1:] +=  -1 * l1_der * self.C1\n",
    "            vk = curr_grad - self.last_grad\n",
    "            inv_hessian_num_1_1 = (sk.T @ vk) + self.inv_hessian\n",
    "            inv_hessian_num_1_2 = sk @ sk.T\n",
    "            inv_hessian_num_1 = inv_hessian_num_1_1 @ inv_hessian_num_1_2\n",
    "            inv_hessian_dom_1 = (sk.T @ vk) ** 2\n",
    "            inv_hessian_1 = inv_hessian_num_1 / inv_hessian_dom_1\n",
    "            inv_hessian_num_2 = (self.inv_hessian @ vk @ sk.T) + (sk @ vk.T @ self.inv_hessian)\n",
    "            inv_hessian_dom_2 =  sk.T @ vk\n",
    "            inv_hessian_2 = inv_hessian_num_2 / inv_hessian_dom_2\n",
    "            self.inv_hessian += inv_hessian_1 - inv_hessian_2\n",
    "            self.last_grad = curr_grad\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 2.11584207e-02]\n",
      " [ 8.52009260e-05]\n",
      " [ 1.59547691e-03]\n",
      " [-8.50315390e-03]\n",
      " [ 2.37194772e-02]\n",
      " [-3.82476915e-03]\n",
      " [ 8.23154481e-02]\n",
      " [-7.84367524e-02]\n",
      " [-1.86783422e-02]\n",
      " [ 8.52503403e-05]\n",
      " [ 8.52211774e-04]\n",
      " [ 4.32540742e-04]\n",
      " [-2.75024265e-03]\n",
      " [-4.16710575e-02]\n",
      " [-2.94371724e-01]\n",
      " [-4.16714908e-02]\n",
      " [ 9.18582164e-02]\n",
      " [ 1.01534898e-02]]\n",
      "0.9283688817655036\n"
     ]
    }
   ],
   "source": [
    "test_model = BFGSFromScratchLogisticRegression(iterations=1000, eta=0.000001)\n",
    "test_model.fit(X_train, y_train)\n",
    "print(test_model)\n",
    "print(accuracy_score(y_test, test_model.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta=.1, iters=10, C1=.001, C2=.0001, solver=\"default\", regularization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.solver = solver\n",
    "        self.classifiers = []\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get number of unique values of y\n",
    "        unique_classes = np.unique(y)\n",
    "        unique_classes.sort()\n",
    "        for target in unique_classes:\n",
    "            # Transform the data into binary classification, the taget class vs the rest\n",
    "            y_binary = np.where(y == target, 1, 0)\n",
    "            if self.solver == \"default\":\n",
    "                model = LogisticRegressionBase(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"sgd\":\n",
    "                model = LogisticRegressionSGD(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"newton\":\n",
    "                model = LogisticRegressionNewtons(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs':\n",
    "                model = BFGSLogisticRegression(iteratins=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs_scratch':\n",
    "                model = BFGSFromScratchLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            model.fit(X, y_binary, regularization=self.regularization)\n",
    "            self.classifiers.append(model)\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers]).T\n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for model in self.classifiers:\n",
    "            probs.append(model.predict_proba(X).reshape(len(X), 1))\n",
    "        return np.hstack(probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(iters=150000, eta=1, solver=\"sgd\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9939588213537172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy of y_hat vs y_test with sklearn\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 5 µs, total: 8 µs\n",
      "Wall time: 4.77 µs\n",
      "Accuracy:  0.814\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(iters=10, eta=.1, solver=\"newton\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat_new = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat_new))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 4 µs, total: 7 µs\n",
      "Wall time: 3.1 µs\n",
      "0.95485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "SKmodel = SKLogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
    "SKmodel.fit(X_train, y_train)\n",
    "%time\n",
    "print(accuracy_score(y_true=y_test, y_pred=SKmodel.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
