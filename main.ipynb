{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('star_classification.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get rid of all rows which have class as GALAXY\n",
    "df = df[df['class'] != 'GALAXY']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('class', axis=1), df['class'], test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# encode the labels:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegressionBase:\n",
    "    def __init__(self, eta=0.1, iterations=20, C1=0.0001, C2 = 0.0001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "    def __str__(self):\n",
    "        if(hasattr(self, 'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    @staticmethod\n",
    "    def _add_bias(X):   \n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return expit(theta)\n",
    "    def _get_gradient(self, X, y):\n",
    "        ydiff = y - self.predict_proba(X, add_bias = False).ravel()\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return gradient\n",
    "    def _get_gradient_L2(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return gradient\n",
    "    def _get_gradient_L1(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * self.C1\n",
    "        return gradient\n",
    "    def _get_gradient_elastic(self, X, y):\n",
    "        gradient = self._get_gradient(X, y)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * self.C1\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return gradient\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) > 0.5\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.random.uniform(-1, 1, (num_features, 1))\n",
    "        for i in range(self.iters):\n",
    "            if(regularization == 'L1'):\n",
    "                grad = self._get_gradient_L1(Xb, y)\n",
    "            elif(regularization == 'L2'):\n",
    "                grad = self._get_gradient_L2(Xb, y)\n",
    "            elif(regularization == 'elastic'):\n",
    "                grad = self._get_gradient_elastic(Xb, y)\n",
    "            else:\n",
    "                grad = self._get_gradient(Xb, y)\n",
    "            self.w_ += grad*self.eta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegressionBase(eta=0.1, iterations=1000)\n",
    "model.fit(X_train, y_train, regularization='elastic')\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegressionSGD(LogisticRegressionBase):\n",
    "    def _get_gradient(self, X, y):\n",
    "        sample = int(np.random.rand()  * len(y))\n",
    "        ydiff = y[sample] - self.predict_proba(X[sample],add_bias=False)\n",
    "        gradient = X[sample] * ydiff[:, np.newaxis]\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "\n",
    "        return gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegressionNewtons(LogisticRegressionBase):\n",
    "    def _get_gradient(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_L1(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X # the second derivative of abs(x) evaluates to 0 so our hessian will simply be the one for the ordinary log likelihood\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X @ ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        gradient[1:] += -1 * l1_der[1:] * self.C1\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_L2(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X - 2 * self.C2\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return np.linalg.pinv(hessian) @ gradient\n",
    "    def _get_gradient_elastic(self, X, y):\n",
    "        g = self.predict_proba(X, add_bias=False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1-g)) @ X - 2 * self.C2\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X @ ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_)\n",
    "        l1_der = self.w_ / np.abs(self.w_)\n",
    "        gradient[1:] += -1 * l1_der[1:] * self.C1\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C2\n",
    "        return np.linalg.pinv(hessian) @ gradient\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegressionNewtons(eta=0.1, iterations=10)\n",
    "model.fit(X_train, y_train, regularization='L2')\n",
    "y_pred = model.predict(X_test)\n",
    "print(model)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import ma\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSLogisticRegression(LogisticRegressionBase):\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C1,C2):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C2*sum(w**2) + C1*sum(np.abs(w))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C1, C2):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C2\n",
    "        l1_der = w[1:] / np.abs(w[1:])\n",
    "        l1_der[w[1:] == 0] = 0\n",
    "        gradient[1:] +=  -1 * l1_der * C1\n",
    "\n",
    "        return -gradient\n",
    "\n",
    "\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        #modifying regularization params so we can use one objective function and gradient in all cases\n",
    "        if(regularization == 'L1'):\n",
    "            self.C2 = 0\n",
    "        elif(regularization == 'L2'):\n",
    "            self.C1 = 0\n",
    "        elif(regularization == 'elastic'):\n",
    "            pass\n",
    "        else:\n",
    "            self.C1 = 0\n",
    "            self.C2 = 0\n",
    "\n",
    "        self.w_ = fmin_bfgs(self.objective_function,\n",
    "                        np.zeros((num_features,1)),\n",
    "                        fprime=self.objective_gradient,\n",
    "                        args=(Xb,y, self.C1, self.C2),\n",
    "                        gtol=1e-03,\n",
    "                        maxiter=self.iters,\n",
    "                        disp=False)\n",
    "\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bfgslr = BFGSLogisticRegression(iterations=200, C2=0.001, C1=0.001)\n",
    "bfgslr.fit(X_train, y_train, regularization='elastic')\n",
    "print(bfgslr)\n",
    "print(accuracy_score(y_test, bfgslr.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BFGSFromScratchLogisticRegression(LogisticRegressionBase):\n",
    "\n",
    "\n",
    "    def fit(self, X, y, regularization=None):\n",
    "        if(regularization == 'L1'):\n",
    "            self.C2 = 0\n",
    "        elif(regularization == 'L2'):\n",
    "            self.C1 = 0\n",
    "        elif(regularization == 'elastic'):\n",
    "            pass\n",
    "        else:\n",
    "            self.C1 = 0\n",
    "            self.C2 = 0\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        self.inv_hessian = np.identity(num_features)\n",
    "        self.last_grad = np.zeros((num_features,1))\n",
    "        g = self.predict_proba(Xb, add_bias=False).ravel()\n",
    "        ydiff = y-g\n",
    "        self.last_grad = np.sum(Xb * ydiff[:, np.newaxis], axis=0)\n",
    "        self.last_grad = self.last_grad.reshape((num_features, 1))\n",
    "        self.last_grad[1:] += -2 * self.w_[1:] * self.C2\n",
    "        l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "        l1_der[self.w_[1:] == 0] = 0\n",
    "        self.last_grad[1:] +=  -1 * l1_der * self.C1\n",
    "        self.last_grad = -self.last_grad\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            pk = -np.dot(self.inv_hessian, self.last_grad)\n",
    "            pk = pk.reshape((num_features, 1))\n",
    "            sk = self.eta * pk\n",
    "            self.w_ += sk\n",
    "            g = self.predict_proba(Xb, add_bias=False).ravel()\n",
    "            ydiff = y-g\n",
    "            curr_grad = np.sum(Xb * ydiff[:, np.newaxis], axis=0)\n",
    "            curr_grad = curr_grad.reshape((num_features, 1))\n",
    "            curr_grad[1:] += -2 * self.w_[1:] * self.C2\n",
    "            l1_der = self.w_[1:] / np.abs(self.w_[1:])\n",
    "            l1_der[self.w_[1:] == 0] = 0\n",
    "            curr_grad[1:] +=  -1 * l1_der * self.C1\n",
    "            curr_grad = -curr_grad\n",
    "            vk = curr_grad - self.last_grad\n",
    "            inv_hessian_num_1_1 = (sk.T @ vk) + self.inv_hessian\n",
    "            inv_hessian_num_1_2 = sk @ sk.T\n",
    "            inv_hessian_num_1 = inv_hessian_num_1_1 @ inv_hessian_num_1_2\n",
    "            inv_hessian_dom_1 = (sk.T @ vk) ** 2\n",
    "            inv_hessian_1 = inv_hessian_num_1 / inv_hessian_dom_1\n",
    "            inv_hessian_num_2 = (self.inv_hessian @ vk @ sk.T) + (sk @ vk.T @ self.inv_hessian)\n",
    "            inv_hessian_dom_2 =  sk.T @ vk\n",
    "            inv_hessian_2 = inv_hessian_num_2 / inv_hessian_dom_2\n",
    "            self.inv_hessian += inv_hessian_1 - inv_hessian_2\n",
    "            self.last_grad = curr_grad\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_model = BFGSFromScratchLogisticRegression(iterations=100, eta=0.001)\n",
    "test_model.fit(X_train, y_train, regularization='elastic')\n",
    "print(test_model)\n",
    "print(accuracy_score(y_test, test_model.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta=.1, iters=10, C1=.001, C2=.0001, solver=\"default\", regularization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.solver = solver\n",
    "        self.classifiers = []\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get number of unique values of y\n",
    "        unique_classes = np.unique(y)\n",
    "        unique_classes.sort()\n",
    "        for target in unique_classes:\n",
    "            # Transform the data into binary classification, the taget class vs the rest\n",
    "            y_binary = np.where(y == target, 1, 0)\n",
    "            if self.solver == \"default\":\n",
    "                model = LogisticRegressionBase(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"sgd\":\n",
    "                model = LogisticRegressionSGD(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"newton\":\n",
    "                model = LogisticRegressionNewtons(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs':\n",
    "                model = BFGSLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs_scratch':\n",
    "                model = BFGSFromScratchLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            model.fit(X, y_binary, regularization=self.regularization)\n",
    "            self.classifiers.append(model)\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers]).T\n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for model in self.classifiers:\n",
    "            probs.append(model.predict_proba(X).reshape(len(X), 1))\n",
    "        return np.hstack(probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegression(iters=150000, eta=1, solver=\"sgd\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy of y_hat vs y_test with sklearn\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LogisticRegression(iters=10, eta=.1, solver=\"newton\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat_new = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat_new))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "SKmodel = SKLogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
    "SKmodel.fit(X_train, y_train)\n",
    "%time\n",
    "print(accuracy_score(y_true=y_test, y_pred=SKmodel.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid Search for best parameters\n",
    "params = {\"C\": [.001, .01, .1], 'iters': [5, 50, 500, 5000], 'solver':['sgd', 'bfgs', \"default\"]}\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# create all combos of params:\n",
    "param_combos = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for params in param_combos:\n",
    "    model = LogisticRegression(iters=params[\"iters\"], eta=.1, C1=params['C'], C2=params['C'], regularization=\"elastic\", solver=params['solver'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat_new = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_hat_new))\n",
    "\n",
    "for i in range(len(param_combos)):\n",
    "    print(\"Accuracy for\", param_combos[i], \"is\", accuracies[i])\n",
    "\n",
    "# Print out the grid search results in a table with C as the row and iters as the column\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The columns represent the number of iterations and the rows represent the regularization terms. We decided to use 5,50,500 and 5000 iterations for the regularizations terms for elasticnet regression .0001, 0.01, and .1. We could've tried using L1 or L2 regularization seperately but felt that ElasticNet Regression gave us the best bang for our buck, and for simplicity, just to keep the regularization constant C for each as the same value.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_combo_accuracies = []\n",
    "for i in range(len(param_combos)):\n",
    "    param_combo_accuracies.append({\"C\": param_combos[i][\"C\"], \"iters\": param_combos[i][\"iters\"], \"solver\": param_combos[i][\"solver\"], \"accuracy\": accuracies[i]})\n",
    "\n",
    "param_combo_accuracies.sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(param_combo_accuracies[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ -8.88627572]\n",
      " [  5.27509514]\n",
      " [  0.90772017]\n",
      " [ -0.53636115]\n",
      " [  4.44592337]\n",
      " [  2.95011535]\n",
      " [ -9.16086727]\n",
      " [-17.01320358]\n",
      " [  2.19833927]\n",
      " [  5.27584254]\n",
      " [  9.46448486]\n",
      " [  2.3352807 ]\n",
      " [  1.83822263]\n",
      " [ -1.66595187]\n",
      " [-89.71738737]\n",
      " [ -1.66584169]\n",
      " [  3.93691556]\n",
      " [  1.63839714]]\n",
      "0.9562322771544816\n"
     ]
    }
   ],
   "source": [
    "test_model = BFGSFromScratchLogisticRegression(iterations=100, eta=0.001)\n",
    "test_model.fit(X_train, y_train, regularization='elastic')\n",
    "print(test_model)\n",
    "print(accuracy_score(y_test, test_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta=.1, iters=10, C1=.001, C2=.0001, solver=\"default\", regularization=None):\n",
    "        self.eta = eta\n",
    "        self.iters = iters\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.solver = solver\n",
    "        self.classifiers = []\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get number of unique values of y\n",
    "        unique_classes = np.unique(y)\n",
    "        unique_classes.sort()\n",
    "        for target in unique_classes:\n",
    "            # Transform the data into binary classification, the taget class vs the rest\n",
    "            y_binary = np.where(y == target, 1, 0)\n",
    "            if self.solver == \"default\":\n",
    "                model = LogisticRegressionBase(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"sgd\":\n",
    "                model = LogisticRegressionSGD(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == \"newton\":\n",
    "                model = LogisticRegressionNewtons(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs':\n",
    "                model = BFGSLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            elif self.solver == 'bfgs_scratch':\n",
    "                model = BFGSFromScratchLogisticRegression(iterations=self.iters, eta=self.eta, C1=self.C1, C2=self.C2)\n",
    "            model.fit(X, y_binary, regularization=self.regularization)\n",
    "            self.classifiers.append(model)\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers]).T\n",
    "    def predict_proba(self, X):\n",
    "        probs = []\n",
    "        for model in self.classifiers:\n",
    "            probs.append(model.predict_proba(X).reshape(len(X), 1))\n",
    "        return np.hstack(probs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return np.argmax(probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(iters=150000, eta=1, solver=\"sgd\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy of y_hat vs y_test with sklearn\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 5 µs, total: 8 µs\n",
      "Wall time: 4.77 µs\n",
      "Accuracy:  0.814\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(iters=10, eta=.1, solver=\"newton\", regularization=\"l2\")\n",
    "model.fit(X_train, y_train)\n",
    "%time\n",
    "y_hat_new = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_hat_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 4 µs, total: 7 µs\n",
      "Wall time: 3.1 µs\n",
      "0.95485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "SKmodel = SKLogisticRegression(solver='lbfgs', penalty='l2', max_iter=1000)\n",
    "SKmodel.fit(X_train, y_train)\n",
    "%time\n",
    "print(accuracy_score(y_true=y_test, y_pred=SKmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n",
      "<ipython-input-12-2ab07835cb08>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  l1_der = w[1:] / np.abs(w[1:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for {'C': 0.001, 'iters': 5, 'solver': 'sgd'} is 0.6700776722968808\n",
      "Accuracy for {'C': 0.001, 'iters': 5, 'solver': 'bfgs'} is 0.9441499198619159\n",
      "Accuracy for {'C': 0.001, 'iters': 5, 'solver': 'default'} is 0.29466157070644805\n",
      "Accuracy for {'C': 0.001, 'iters': 50, 'solver': 'sgd'} is 0.8468746147207497\n",
      "Accuracy for {'C': 0.001, 'iters': 50, 'solver': 'bfgs'} is 0.9749722598939712\n",
      "Accuracy for {'C': 0.001, 'iters': 50, 'solver': 'default'} is 0.8926149673283196\n",
      "Accuracy for {'C': 0.001, 'iters': 500, 'solver': 'sgd'} is 0.955369251633584\n",
      "Accuracy for {'C': 0.001, 'iters': 500, 'solver': 'bfgs'} is 0.9749722598939712\n",
      "Accuracy for {'C': 0.001, 'iters': 500, 'solver': 'default'} is 0.962643323881149\n",
      "Accuracy for {'C': 0.001, 'iters': 5000, 'solver': 'sgd'} is 0.9779312045370484\n",
      "Accuracy for {'C': 0.001, 'iters': 5000, 'solver': 'bfgs'} is 0.9749722598939712\n",
      "Accuracy for {'C': 0.001, 'iters': 5000, 'solver': 'default'} is 0.9781777832573049\n",
      "Accuracy for {'C': 0.01, 'iters': 5, 'solver': 'sgd'} is 0.753914437184071\n",
      "Accuracy for {'C': 0.01, 'iters': 5, 'solver': 'bfgs'} is 0.9452595241030699\n",
      "Accuracy for {'C': 0.01, 'iters': 5, 'solver': 'default'} is 0.7430649734927876\n",
      "Accuracy for {'C': 0.01, 'iters': 50, 'solver': 'sgd'} is 0.8825052397978055\n",
      "Accuracy for {'C': 0.01, 'iters': 50, 'solver': 'bfgs'} is 0.9594378005178154\n",
      "Accuracy for {'C': 0.01, 'iters': 50, 'solver': 'default'} is 0.8823819504376772\n",
      "Accuracy for {'C': 0.01, 'iters': 500, 'solver': 'sgd'} is 0.9572185920355073\n",
      "Accuracy for {'C': 0.01, 'iters': 500, 'solver': 'bfgs'} is 0.9594378005178154\n",
      "Accuracy for {'C': 0.01, 'iters': 500, 'solver': 'default'} is 0.958698064357046\n",
      "Accuracy for {'C': 0.01, 'iters': 5000, 'solver': 'sgd'} is 0.9511774133892245\n",
      "Accuracy for {'C': 0.01, 'iters': 5000, 'solver': 'bfgs'} is 0.9594378005178154\n",
      "Accuracy for {'C': 0.01, 'iters': 5000, 'solver': 'default'} is 0.9612871409197387\n",
      "Accuracy for {'C': 0.1, 'iters': 5, 'solver': 'sgd'} is 0.4407594624583898\n",
      "Accuracy for {'C': 0.1, 'iters': 5, 'solver': 'bfgs'} is 0.8592035507335717\n",
      "Accuracy for {'C': 0.1, 'iters': 5, 'solver': 'default'} is 0.3727037356676119\n",
      "Accuracy for {'C': 0.1, 'iters': 50, 'solver': 'sgd'} is 0.8473677721612625\n",
      "Accuracy for {'C': 0.1, 'iters': 50, 'solver': 'bfgs'} is 0.8592035507335717\n",
      "Accuracy for {'C': 0.1, 'iters': 50, 'solver': 'default'} is 0.8922450992479349\n",
      "Accuracy for {'C': 0.1, 'iters': 500, 'solver': 'sgd'} is 0.8942177290099864\n",
      "Accuracy for {'C': 0.1, 'iters': 500, 'solver': 'bfgs'} is 0.8592035507335717\n",
      "Accuracy for {'C': 0.1, 'iters': 500, 'solver': 'default'} is 0.9326840093699914\n",
      "Accuracy for {'C': 0.1, 'iters': 5000, 'solver': 'sgd'} is 0.9029712735790901\n",
      "Accuracy for {'C': 0.1, 'iters': 5000, 'solver': 'bfgs'} is 0.8592035507335717\n",
      "Accuracy for {'C': 0.1, 'iters': 5000, 'solver': 'default'} is 0.9321908519294785\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for best parameters\n",
    "params = {\"C\": [.001, .01, .1], 'iters': [5, 50, 500, 5000], 'solver':['sgd', 'bfgs', \"default\"]}\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "# create all combos of params:\n",
    "param_combos = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for params in param_combos:\n",
    "    model = LogisticRegression(iters=params[\"iters\"], eta=.1, C1=params['C'], C2=params['C'], regularization=\"elastic\", solver=params['solver'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat_new = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_hat_new))\n",
    "\n",
    "for i in range(len(param_combos)):\n",
    "    print(\"Accuracy for\", param_combos[i], \"is\", accuracies[i])\n",
    "\n",
    "# Print out the grid search results in a table with C as the row and iters as the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>50</th>\n",
       "      <th>500</th>\n",
       "      <th>5000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.553076</td>\n",
       "      <td>0.842190</td>\n",
       "      <td>0.962520</td>\n",
       "      <td>0.978178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.691283</td>\n",
       "      <td>0.874985</td>\n",
       "      <td>0.958451</td>\n",
       "      <td>0.961410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.695722</td>\n",
       "      <td>0.949821</td>\n",
       "      <td>0.933177</td>\n",
       "      <td>0.933177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           5         50        500       5000\n",
       "0.001  0.553076  0.842190  0.962520  0.978178\n",
       "0.010  0.691283  0.874985  0.958451  0.961410\n",
       "0.100  0.695722  0.949821  0.933177  0.933177"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns represent the number of iterations and the rows represent the regularization terms. We decided to use 5,50,500 and 5000 iterations for the regularizations terms for elasticnet regression .0001, 0.01, and .1. We could've tried using L1 or L2 regularization seperately but felt that ElasticNet Regression gave us the best bang for our buck, and for simplicity, just to keep the regularization constant C for each as the same value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001, 'iters': 5000, 'solver': 'default', 'accuracy': 0.9781777832573049}\n",
      "{'C': 0.001, 'iters': 5000, 'solver': 'sgd', 'accuracy': 0.9779312045370484}\n",
      "{'C': 0.001, 'iters': 50, 'solver': 'bfgs', 'accuracy': 0.9749722598939712}\n",
      "{'C': 0.001, 'iters': 500, 'solver': 'bfgs', 'accuracy': 0.9749722598939712}\n",
      "{'C': 0.001, 'iters': 5000, 'solver': 'bfgs', 'accuracy': 0.9749722598939712}\n"
     ]
    }
   ],
   "source": [
    "param_combo_accuracies = []\n",
    "for i in range(len(param_combos)):\n",
    "    param_combo_accuracies.append({\"C\": param_combos[i][\"C\"], \"iters\": param_combos[i][\"iters\"], \"solver\": param_combos[i][\"solver\"], \"accuracy\": accuracies[i]})\n",
    "\n",
    "param_combo_accuracies.sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(param_combo_accuracies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}